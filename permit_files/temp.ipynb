{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_id: PDS2013-RESALT-007197\n",
      "parcel_number: 594-240-39-00\n",
      "primary_scope_code: 8002 - REN - (Online) RES Roof Mt Solar PV No Meter Upgrade (HRA)\n",
      "valuation: \n",
      "record_id: PDS2013-RESALT-007200\n",
      "parcel_number: 590-441-16-00\n",
      "primary_scope_code: 8002 - REN - (Online) RES Roof Mt Solar PV No Meter Upgrade (HRA)\n",
      "valuation: \n"
     ]
    }
   ],
   "source": [
    "# Without separation between records\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Function to query the dataset based on user input\n",
    "def query_permit_data(dataset_path, user_input):\n",
    "    # Read the CSV dataset\n",
    "    data = []\n",
    "    with open(dataset_path, 'r', newline='') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            data.append(row)\n",
    "\n",
    "    # Split the user input into query components\n",
    "    query_components = re.split(r'\\s+', user_input)\n",
    "    select_clause = query_components[1:query_components.index('from')]\n",
    "    from_clause = query_components[query_components.index('from') + 1]\n",
    "    where_clause = ' '.join(query_components[query_components.index('where') + 1:])\n",
    "\n",
    "    # Filter the dataset based on the WHERE clause\n",
    "    filtered_data = []\n",
    "    for row in data:\n",
    "        if evaluate_condition(row, where_clause):\n",
    "            filtered_data.append(row)\n",
    "\n",
    "    # Extract the SELECTed columns from the filtered data\n",
    "    result = []\n",
    "    for row in filtered_data:\n",
    "        selected_data = {}\n",
    "        for column in select_clause:\n",
    "            selected_data[column] = row[column]\n",
    "        result.append(selected_data)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Function to evaluate the WHERE condition\n",
    "def evaluate_condition(row, condition):\n",
    "    operators = ['>', '<', '>=', '<=', '=', '!=']\n",
    "    for operator in operators:\n",
    "        if operator in condition:\n",
    "            column, value = condition.split(operator)\n",
    "            column = column.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if operator == '=':\n",
    "                operator = '=='\n",
    "\n",
    "            if column in row and eval(f'\"{row[column]}\" {operator} \"{value}\"'):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "dataset_path = 'xpermit_data.csv'\n",
    "user_input = \"select record_id parcel_number primary_scope_code valuation from data where primary_scope_code = 8002 - REN - (Online) RES Roof Mt Solar PV No Meter Upgrade (HRA)\"\n",
    "results = query_permit_data(dataset_path, user_input)\n",
    "for result in results:\n",
    "    for column, value in result.items():\n",
    "        print(f'{column}: {value}')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_id: PDS2013-MHPARK-000239\n",
      "parcel_number: 185-332-09-66\n",
      "primary_scope_code: 1158 - M.H.P. - 902 - Installation - New Or Existing\n",
      "valuation: \n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007205\n",
      "parcel_number: 104-412-03-00\n",
      "primary_scope_code: 8020 - ACC - Mechanical Only\n",
      "valuation: 12601.08\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007202\n",
      "parcel_number: 379-171-02-00\n",
      "primary_scope_code: 8040 - ACC - Electrical & Mechanical\n",
      "valuation: 8874\n",
      "--------------------\n",
      "record_id: PDS2013-RESACC-000628\n",
      "parcel_number: 266-360-22-00\n",
      "primary_scope_code: 3313 - ACC - Retaining Walls\n",
      "valuation: \n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007208\n",
      "parcel_number: 222-501-03-00\n",
      "primary_scope_code: 8020 - ACC - Mechanical Only\n",
      "valuation: 11567.52\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007188\n",
      "parcel_number: 217-031-09-00\n",
      "primary_scope_code: 4340 - RES - Addition/Alter To SFD/Duplex\n",
      "valuation: 85034.84\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007180\n",
      "parcel_number: 491-520-41-00\n",
      "primary_scope_code: 8000 - ACC - Electrical Only\n",
      "valuation: \n",
      "--------------------\n",
      "record_id: PDS2013-COMALT-000671\n",
      "parcel_number: 678-291-35-00\n",
      "primary_scope_code: 8000 - ACC - Electrical Only\n",
      "valuation: \n",
      "--------------------\n",
      "record_id: PDS2008-1001-20080170\n",
      "parcel_number: 606-150-30-00\n",
      "primary_scope_code: 1010 - RES - New Primary Residential Structure\n",
      "valuation: 499101.78\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007218\n",
      "parcel_number: 590-420-07-00\n",
      "primary_scope_code: 3310 - RES - Misc. (Antennas, Tennis Cts, Gazebo)\n",
      "valuation: 1530\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007194\n",
      "parcel_number: 509-150-57-00\n",
      "primary_scope_code: 8003 - REN - Solar Photovoltaic Residential (HRA)\n",
      "valuation: \n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007215\n",
      "parcel_number: 506-061-08-00\n",
      "primary_scope_code: 8000 - ACC - Electrical Only\n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n"
     ]
    }
   ],
   "source": [
    "# With separation between records\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Function to query the dataset based on user input\n",
    "def query_permit_data(dataset_path, user_input):\n",
    "    # Read the CSV dataset\n",
    "    data = []\n",
    "    with open(dataset_path, 'r', newline='') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            data.append(row)\n",
    "\n",
    "    # Split the user input into query components\n",
    "    query_components = re.split(r'\\s+', user_input)\n",
    "    select_clause = query_components[1:query_components.index('from')]\n",
    "    from_clause = query_components[query_components.index('from') + 1]\n",
    "    where_clause = ' '.join(query_components[query_components.index('where') + 1:])\n",
    "\n",
    "    # Filter the dataset based on the WHERE clause\n",
    "    filtered_data = []\n",
    "    for row in data:\n",
    "        if evaluate_condition(row, where_clause):\n",
    "            filtered_data.append(row)\n",
    "\n",
    "    # Extract the SELECTed columns from the filtered data\n",
    "    result = []\n",
    "    for row in filtered_data:\n",
    "        selected_data = {}\n",
    "        for column in select_clause:\n",
    "            selected_data[column] = row[column]\n",
    "        result.append(selected_data)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Function to evaluate the WHERE condition\n",
    "def evaluate_condition(row, condition):\n",
    "    operators = ['>', '<', '>=', '<=', '=', '!=']\n",
    "    for operator in operators:\n",
    "        if operator in condition:\n",
    "            column, value = condition.split(operator)\n",
    "            column = column.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if operator == '=':\n",
    "                operator = '=='\n",
    "\n",
    "            if column in row and eval(f'\"{row[column]}\" {operator} \"{value}\"'):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "dataset_path = 'xpermit_data.csv'\n",
    "user_input = \"select record_id parcel_number primary_scope_code valuation from data where primary_scope_code != 8002 - REN - (Online) RES Roof Mt Solar PV No Meter Upgrade (HRA)\"\n",
    "results = query_permit_data(dataset_path, user_input)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    for column, value in result.items():\n",
    "        print(f'{column}: {value}')\n",
    "    # Add a separator between records except for the last one\n",
    "    if i < len(results) - 1:\n",
    "        print('-' * 20)  # Use any separator you prefer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_id: PDS2013-RESALT-007197\n",
      "parcel_number: 594-240-39-00\n",
      "primary_scope_code: 8002 - REN - (Online) RES Roof Mt Solar PV No Meter Upgrade (HRA)\n",
      "valuation: \n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007200\n",
      "parcel_number: 590-441-16-00\n",
      "primary_scope_code: 8002 - REN - (Online) RES Roof Mt Solar PV No Meter Upgrade (HRA)\n",
      "valuation: \n"
     ]
    }
   ],
   "source": [
    "# Optional WHERE clause - WORKING\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Function to query the dataset based on user input\n",
    "def query_permit_data(dataset_path, user_input):\n",
    "    # Read the CSV dataset\n",
    "    data = []\n",
    "    with open(dataset_path, 'r', newline='') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            data.append(row)\n",
    "\n",
    "    # Split the user input into query components\n",
    "    query_components = re.split(r'\\s+', user_input)\n",
    "    select_clause = query_components[1:query_components.index('from')]\n",
    "    from_clause = query_components[query_components.index('from') + 1]\n",
    "    where_index = query_components.index('where') if 'where' in query_components else None\n",
    "\n",
    "    # Filter the dataset based on the WHERE clause if present\n",
    "    filtered_data = data\n",
    "    if where_index is not None:\n",
    "        where_clause = ' '.join(query_components[where_index + 1:])\n",
    "        filtered_data = [row for row in data if evaluate_condition(row, where_clause)]\n",
    "\n",
    "    # Extract the SELECTed columns from the filtered data\n",
    "    result = []\n",
    "    for row in filtered_data:\n",
    "        selected_data = {}\n",
    "        for column in select_clause:\n",
    "            selected_data[column] = row[column]\n",
    "        result.append(selected_data)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Function to evaluate the WHERE condition\n",
    "def evaluate_condition(row, condition):\n",
    "    operators = ['>', '<', '>=', '<=', '=', '!=']\n",
    "    for operator in operators:\n",
    "        if operator in condition:\n",
    "            column, value = condition.split(operator)\n",
    "            column = column.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if operator == '=':\n",
    "                operator = '=='\n",
    "\n",
    "            if '*' in value:\n",
    "                # If the value contains \"*\", perform wildcard matching\n",
    "                value_pattern = value.replace('*', '.*')\n",
    "                if re.match(value_pattern, row.get(column)) is not None:\n",
    "                    return True\n",
    "            else:\n",
    "                # Perform standard comparison\n",
    "                if column in row and eval(f'\"{row[column]}\" {operator} \"{value}\"'):\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "dataset_path = 'xpermit_data.csv'\n",
    "user_input = \"select record_id parcel_number primary_scope_code valuation from data where primary_scope_code = 8002 - REN - (Online) RES Roof Mt Solar PV No Meter Upgrade (HRA)\"\n",
    "# user_input = \"select record_id parcel_number primary_scope_code valuation from data where parcel_number = 594-240-39-00\"\n",
    "results = query_permit_data(dataset_path, user_input)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    for column, value in result.items():\n",
    "        print(f'{column}: {value}')\n",
    "    # Add a separator between records except for the last one\n",
    "    if i < len(results) - 1:\n",
    "        print('-' * 20)  # Use any separator you prefer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_id: PDS2013-RESALT-007197\n",
      "record_status: Issued Invalid Expired\n",
      "--------------------\n",
      "record_id: PDS2013-MHPARK-000239\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007205\n",
      "record_status: Issued Invalid Expired\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007202\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2013-RESACC-000628\n",
      "record_status: Issued Expired\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007208\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007188\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007180\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2013-COMALT-000671\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2008-1001-20080170\n",
      "record_status: PC Expired\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007218\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007194\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007200\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007215\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# 11/13/ - GROUP BY - Working, shows count\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Function to query the dataset based on user input\n",
    "def query_permit_data(dataset_path, user_input, group_by=None):\n",
    "    # Read the CSV dataset\n",
    "    data = []\n",
    "    with open(dataset_path, 'r', newline='') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            data.append(row)\n",
    "\n",
    "    # Split the user input into query components\n",
    "    query_components = re.split(r'\\s+', user_input)\n",
    "    select_clause = query_components[1:query_components.index('from')]\n",
    "    from_clause = query_components[query_components.index('from') + 1]\n",
    "    where_index = query_components.index('where') if 'where' in query_components else None\n",
    "\n",
    "    # Filter the dataset based on the WHERE clause if present\n",
    "    filtered_data = data\n",
    "    if where_index is not None:\n",
    "        where_clause = ' '.join(query_components[where_index + 1:])\n",
    "        filtered_data = [row for row in data if evaluate_condition(row, where_clause)]\n",
    "\n",
    "    # Extract the SELECTed columns from the filtered data\n",
    "    selected_data = []\n",
    "    for row in filtered_data:\n",
    "        row_data = {}\n",
    "        for column in select_clause:\n",
    "            row_data[column] = row[column]\n",
    "        selected_data.append(row_data)\n",
    "\n",
    "    if group_by is not None:\n",
    "        grouped_results = group_by_column(selected_data, group_by)\n",
    "    else:\n",
    "        grouped_results = selected_data\n",
    "\n",
    "    return grouped_results\n",
    "\n",
    "# Function to evaluate the WHERE condition\n",
    "def evaluate_condition(row, condition):\n",
    "    operators = ['>', '<', '>=', '<=', '=', '!=']\n",
    "    for operator in operators:\n",
    "        if operator in condition:\n",
    "            column, value = condition.split(operator)\n",
    "            column = column.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if operator == '=':\n",
    "                operator = '=='\n",
    "\n",
    "            if '*' in value:\n",
    "                # If the value contains \"*\", perform wildcard matching\n",
    "                value_pattern = value.replace('*', '.*')\n",
    "                if re.match(value_pattern, row.get(column)) is not None:\n",
    "                    return True\n",
    "            else:\n",
    "                # Perform standard comparison with conversion for \"valuation\" column\n",
    "                if column in row:\n",
    "                    if column == \"valuation\":\n",
    "                        try:\n",
    "                            row_value = float(row[column])\n",
    "                            value = float(value)\n",
    "                            if eval(f'{row_value} {operator} {value}'):\n",
    "                                return True\n",
    "                        except ValueError:\n",
    "                            # Handle the case where the \"valuation\" column cannot be converted to a numeric type\n",
    "                            pass\n",
    "                    else:\n",
    "                        if eval(f'\"{row[column]}\" {operator} \"{value}\"'):\n",
    "                            return True\n",
    "    return False\n",
    "\n",
    "# Function to group results by a specified column and count occurrences\n",
    "def group_by_column(data, column_name):\n",
    "    grouped_data = {}\n",
    "    for row in data:\n",
    "        value = row.get(column_name)\n",
    "        if value not in grouped_data:\n",
    "            grouped_data[value] = 1\n",
    "        else:\n",
    "            grouped_data[value] += 1\n",
    "\n",
    "    grouped_results = []\n",
    "    for value, count in grouped_data.items():\n",
    "        grouped_results.append({column_name: value, 'Count': count})  # Include count in the result\n",
    "\n",
    "    return grouped_results\n",
    "\n",
    "# Example usage\n",
    "dataset_path = 'xpermit_data.csv'\n",
    "\n",
    "# user_input = \"select record_id parcel_number open_date primary_scope_code valuation from data where parcel_number = 594-240-39-00\"\n",
    "# user_input = \"select record_id parcel_number primary_scope_code valuation from data where valuation >= 400000\"\n",
    "# user_input = \"select record_id parcel_number primary_scope_code valuation record_category from data where record_category = Residential*\"\n",
    "# user_input = \"select record_status record_id parcel_number primary_scope_code valuation record_category from data where record_status = Com*\"\n",
    "user_input = \"select record_id record_status from data\"\n",
    "group_by_column_name = None\n",
    "\n",
    "# user_input = \"select record_status from data\"\n",
    "# group_by_column_name = 'record_status'  # Set to None if you don't want to group\n",
    "results = query_permit_data(dataset_path, user_input, group_by=group_by_column_name)\n",
    "\n",
    "for result in results:\n",
    "    for column, value in result.items():\n",
    "        print(f'{column}: {value}')\n",
    "    print('-' * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_status: Issued Invalid Expired\n",
      "Count: 0\n",
      "--------------------\n",
      "record_status: Completed\n",
      "Count: 0\n",
      "--------------------\n",
      "record_status: Issued Expired\n",
      "Count: 0\n",
      "--------------------\n",
      "record_status: PC Expired\n",
      "Count: 0\n",
      "--------------------\n",
      "record_status: PC Invalid Expired\n",
      "Count: 0\n",
      "--------------------\n",
      "record_status: Issued About to Expire\n",
      "Count: 0\n",
      "--------------------\n",
      "record_status: Issued\n",
      "Count: 0\n",
      "--------------------\n",
      "record_status: Open\n",
      "Count: 0\n",
      "--------------------\n",
      "record_status: \n",
      "Count: 0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# all aggregations - working\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Function to query the dataset based on user input\n",
    "def query_permit_data(dataset_path, user_input, group_by=None, aggregate_function=None):\n",
    "    # Read the CSV dataset\n",
    "    data = []\n",
    "    with open(dataset_path, 'r', newline='') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            data.append(row)\n",
    "\n",
    "    # Split the user input into query components\n",
    "    query_components = re.split(r'\\s+', user_input)\n",
    "    select_clause = query_components[1:query_components.index('from')]\n",
    "    from_clause = query_components[query_components.index('from') + 1]\n",
    "    where_index = query_components.index('where') if 'where' in query_components else None\n",
    "\n",
    "    # Filter the dataset based on the WHERE clause if present\n",
    "    filtered_data = data\n",
    "    if where_index is not None:\n",
    "        where_clause = ' '.join(query_components[where_index + 1:])\n",
    "        filtered_data = [row for row in data if evaluate_condition(row, where_clause)]\n",
    "\n",
    "    # Extract the SELECTed columns from the filtered data\n",
    "    selected_data = []\n",
    "    for row in filtered_data:\n",
    "        row_data = {}\n",
    "        for column in select_clause:\n",
    "            row_data[column] = row[column]\n",
    "        selected_data.append(row_data)\n",
    "\n",
    "    if group_by is not None:\n",
    "        grouped_results = group_by_column(selected_data, group_by, aggregate_function)\n",
    "    else:\n",
    "        grouped_results = selected_data\n",
    "\n",
    "    return grouped_results\n",
    "\n",
    "# Function to evaluate the WHERE condition\n",
    "def evaluate_condition(row, condition):\n",
    "    operators = ['>', '<', '>=', '<=', '=', '!=']\n",
    "    for operator in operators:\n",
    "        if operator in condition:\n",
    "            column, value = condition.split(operator)\n",
    "            column = column.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if operator == '=':\n",
    "                operator = '=='\n",
    "\n",
    "            if '*' in value:\n",
    "                # If the value contains \"*\", perform wildcard matching\n",
    "                value_pattern = value.replace('*', '.*')\n",
    "                if re.match(value_pattern, row.get(column)) is not None:\n",
    "                    return True\n",
    "            else:\n",
    "                # Perform standard comparison with conversion for \"valuation\" column\n",
    "                if column in row:\n",
    "                    if column == \"valuation\":\n",
    "                        try:\n",
    "                            row_value = float(row[column])\n",
    "                            value = float(value)\n",
    "                            if eval(f'{row_value} {operator} {value}'):\n",
    "                                return True\n",
    "                        except ValueError:\n",
    "                            # Handle the case where the \"valuation\" column cannot be converted to a numeric type\n",
    "                            pass\n",
    "                    else:\n",
    "                        if eval(f'\"{row[column]}\" {operator} \"{value}\"'):\n",
    "                            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def group_by_column(data, column_name, aggregate_function):\n",
    "    grouped_data = {}\n",
    "    for row in data:\n",
    "        key = row.get(column_name)\n",
    "        if key not in grouped_data:\n",
    "            grouped_data[key] = []\n",
    "\n",
    "        # Convert the valuation to a numeric value, or use None if conversion fails\n",
    "        valuation = row.get('valuation')\n",
    "        try:\n",
    "            valuation = float(valuation) if valuation else None\n",
    "        except ValueError:\n",
    "            valuation = None\n",
    "\n",
    "        grouped_data[key].append(valuation)\n",
    "\n",
    "    grouped_results = []\n",
    "    for key, values in grouped_data.items():\n",
    "        # Filter out None values for aggregation\n",
    "        filtered_values = [v for v in values if v is not None]\n",
    "\n",
    "        if aggregate_function:\n",
    "            if aggregate_function == 'max':\n",
    "                result = max(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'min':\n",
    "                result = min(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'avg':\n",
    "                result = np.mean(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'sum':\n",
    "                result = sum(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'count':\n",
    "                result = len(filtered_values)\n",
    "        else:\n",
    "            result = filtered_values\n",
    "\n",
    "        grouped_results.append({column_name: key, aggregate_function.capitalize() if aggregate_function else \"Data\": result})\n",
    "\n",
    "    return grouped_results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "dataset_path = 'permit_data.csv'\n",
    "user_input = \"select record_status record_id from data\"\n",
    "# group_by_column_name = \"record_status\"\n",
    "# aggregate_function = \"min\"  # Change to None for no aggregation or choose 'max', 'min', 'avg', or 'sum'\n",
    "group_by_column_name = 'record_status'\n",
    "aggregate_function = 'count' \n",
    "\n",
    "results = query_permit_data(dataset_path, user_input, group_by=group_by_column_name, aggregate_function=aggregate_function)\n",
    "\n",
    "\n",
    "for result in results:\n",
    "    for column, value in result.items():\n",
    "        print(f'{column}: {value}')\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_status: Completed\n",
      "Count: 27\n",
      "--------------------\n",
      "record_status: Issued Invalid Expired\n",
      "Count: 3\n",
      "--------------------\n",
      "record_status: PC Expired\n",
      "Count: 1\n",
      "--------------------\n",
      "record_status: PC Invalid Expired\n",
      "Count: 1\n",
      "--------------------\n",
      "record_status: Issued Expired\n",
      "Count: 0\n",
      "--------------------\n",
      "record_status: Issued About to Expire\n",
      "Count: 0\n",
      "--------------------\n",
      "record_status: Issued\n",
      "Count: 0\n",
      "--------------------\n",
      "record_status: Open\n",
      "Count: 0\n",
      "--------------------\n",
      "record_status: \n",
      "Count: 0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Count not working; only works with Valuation\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Function to query the dataset based on user input\n",
    "def query_permit_data(dataset_path, user_input, group_by=None, aggregate_function=None, sort_order=None):\n",
    "    # Read the CSV dataset\n",
    "    data = []\n",
    "    # with open(dataset_path, 'r', newline='') as csvfile:\n",
    "    with open(dataset_path, 'r', newline='', encoding='utf-8') as csvfile:  # If utf-8 doesn't work, try 'cp1252' or 'latin1'\n",
    "\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            data.append(row)\n",
    "\n",
    "    # Split the user input into query components\n",
    "    query_components = re.split(r'\\s+', user_input)\n",
    "    select_clause = query_components[1:query_components.index('from')]\n",
    "    from_clause = query_components[query_components.index('from') + 1]\n",
    "    where_index = query_components.index('where') if 'where' in query_components else None\n",
    "\n",
    "    # Filter the dataset based on the WHERE clause if present\n",
    "    filtered_data = data\n",
    "    if where_index is not None:\n",
    "        where_clause = ' '.join(query_components[where_index + 1:])\n",
    "        filtered_data = [row for row in data if evaluate_condition(row, where_clause)]\n",
    "\n",
    "    # Extract the SELECTed columns from the filtered data\n",
    "    selected_data = []\n",
    "    for row in filtered_data:\n",
    "        row_data = {}\n",
    "        for column in select_clause:\n",
    "            row_data[column] = row[column]\n",
    "        selected_data.append(row_data)\n",
    "\n",
    "    if group_by is not None:\n",
    "        grouped_results = group_by_column(selected_data, group_by, aggregate_function)\n",
    "    else:\n",
    "        grouped_results = selected_data\n",
    "\n",
    "    # Sorting the results if sort_order is specified\n",
    "    if sort_order is not None and aggregate_function is not None:\n",
    "        try:\n",
    "            if sort_order.lower() == 'asc':\n",
    "                grouped_results.sort(key=lambda x: (x[aggregate_function.capitalize()] is None, x[aggregate_function.capitalize()]))\n",
    "            elif sort_order.lower() == 'desc':\n",
    "                grouped_results.sort(key=lambda x: (x[aggregate_function.capitalize()] is None, x[aggregate_function.capitalize()]), reverse=True)\n",
    "        except KeyError:\n",
    "            print(\"Invalid sort_order or aggregate_function\")\n",
    "\n",
    "    return grouped_results\n",
    "\n",
    "# Function to evaluate the WHERE condition\n",
    "def evaluate_condition(row, condition):\n",
    "    operators = ['>', '<', '>=', '<=', '=', '!=']\n",
    "    for operator in operators:\n",
    "        if operator in condition:\n",
    "            column, value = condition.split(operator)\n",
    "            column = column.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if operator == '=':\n",
    "                operator = '=='\n",
    "\n",
    "            if '*' in value:\n",
    "                # If the value contains \"*\", perform wildcard matching\n",
    "                value_pattern = value.replace('*', '.*')\n",
    "                if re.match(value_pattern, row.get(column)) is not None:\n",
    "                    return True\n",
    "            else:\n",
    "                # Perform standard comparison with conversion for \"valuation\" column\n",
    "                if column in row:\n",
    "                    if column == \"valuation\":\n",
    "                        try:\n",
    "                            row_value = float(row[column])\n",
    "                            value = float(value)\n",
    "                            if eval(f'{row_value} {operator} {value}'):\n",
    "                                return True\n",
    "                        except ValueError:\n",
    "                            # Handle the case where the \"valuation\" column cannot be converted to a numeric type\n",
    "                            pass\n",
    "                    else:\n",
    "                        if eval(f'\"{row[column]}\" {operator} \"{value}\"'):\n",
    "                            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def group_by_column(data, column_name, aggregate_function):\n",
    "    grouped_data = {}\n",
    "    for row in data:\n",
    "        key = row.get(column_name)\n",
    "        if key not in grouped_data:\n",
    "            grouped_data[key] = []\n",
    "\n",
    "        # Convert the valuation to a numeric value, or use None if conversion fails\n",
    "        valuation = row.get('valuation')\n",
    "        try:\n",
    "            valuation = float(valuation) if valuation else None\n",
    "        except ValueError:\n",
    "            valuation = None\n",
    "\n",
    "        grouped_data[key].append(valuation)\n",
    "\n",
    "    grouped_results = []\n",
    "    for key, values in grouped_data.items():\n",
    "        # Filter out None values for aggregation\n",
    "        filtered_values = [v for v in values if v is not None]\n",
    "\n",
    "        if aggregate_function:\n",
    "            if aggregate_function == 'max':\n",
    "                result = max(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'min':\n",
    "                result = min(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'avg':\n",
    "                result = np.mean(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'sum':\n",
    "                result = sum(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'count':\n",
    "                result = len(filtered_values)\n",
    "        else:\n",
    "            result = filtered_values\n",
    "\n",
    "        grouped_results.append({column_name: key, aggregate_function.capitalize() if aggregate_function else \"Data\": result})\n",
    "\n",
    "    return grouped_results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# dataset_path = 'Building_Permits.csv'\n",
    "dataset_path = 'permit_data.csv'\n",
    "\n",
    "# user_input = \"select record_status valuation from data\"\n",
    "# group_by_column_name = \"record_status\"\n",
    "# aggregate_function = \"avg\"  # Change to None for no aggregation or choose 'max', 'min', 'avg', or 'sum'\n",
    "\n",
    "user_input = \"select record_status valuation from data\"\n",
    "group_by_column_name = \"record_status\"\n",
    "aggregate_function = \"count\"\n",
    "\n",
    "# group_by_column_name = None\n",
    "# aggregate_function = None \n",
    "# sort_order = 'desc'  # Can be 'asc', 'desc', or None. The sorting is applied only if both sort_order and aggregate_function are specified. This is because sorting is typically applied to the aggregated results.\n",
    "results = query_permit_data(dataset_path, user_input, group_by=group_by_column_name, aggregate_function=aggregate_function, sort_order=sort_order)\n",
    "\n",
    "\n",
    "\n",
    "for result in results:\n",
    "    for column, value in result.items():\n",
    "        print(f'{column}: {value}')\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_status: \n",
      "Count: 4\n",
      "--------------------\n",
      "record_status: Issued in Plan Change\n",
      "Count: 105\n",
      "--------------------\n",
      "record_status: PC About to Expire\n",
      "Count: 108\n",
      "--------------------\n",
      "record_status: Issued About to Expire\n",
      "Count: 381\n",
      "--------------------\n",
      "record_status: PC Invalid Expired\n",
      "Count: 2071\n",
      "--------------------\n",
      "record_status: PC Expired\n",
      "Count: 5367\n",
      "--------------------\n",
      "record_status: Issued Expired\n",
      "Count: 8897\n",
      "--------------------\n",
      "record_status: Issued Invalid Expired\n",
      "Count: 10518\n",
      "--------------------\n",
      "record_status: Open\n",
      "Count: 20041\n",
      "--------------------\n",
      "record_status: Issued\n",
      "Count: 55489\n",
      "--------------------\n",
      "record_status: Completed\n",
      "Count: 131434\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Works - 11/15 Wednesday night\n",
    "\n",
    "def query_permit_data(dataset_path, user_input, group_by=None, aggregate_function=None, sort_order=None):\n",
    "    # Read the CSV dataset\n",
    "    data = []\n",
    "    # with open(dataset_path, 'r', newline='') as csvfile:\n",
    "    with open(dataset_path, 'r', newline='', encoding='utf-8') as csvfile:  # If utf-8 doesn't work, try 'cp1252' or 'latin1'\n",
    "\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            data.append(row)\n",
    "\n",
    "    # Split the user input into query components\n",
    "    query_components = re.split(r'\\s+', user_input)\n",
    "    select_clause = query_components[1:query_components.index('from')]\n",
    "    from_clause = query_components[query_components.index('from') + 1]\n",
    "    where_index = query_components.index('where') if 'where' in query_components else None\n",
    "\n",
    "    # Filter the dataset based on the WHERE clause if present\n",
    "    filtered_data = data\n",
    "    if where_index is not None:\n",
    "        where_clause = ' '.join(query_components[where_index + 1:])\n",
    "        filtered_data = [row for row in data if evaluate_condition(row, where_clause)]\n",
    "\n",
    "    # Extract the SELECTed columns from the filtered data\n",
    "    selected_data = []\n",
    "    for row in filtered_data:\n",
    "        row_data = {}\n",
    "        for column in select_clause:\n",
    "            row_data[column] = row[column]\n",
    "        selected_data.append(row_data)\n",
    "\n",
    "    selected_column_name = select_clause[-1]  # Assumes the last item in select_clause is the column to be aggregated\n",
    "\n",
    "    if group_by is not None:\n",
    "        grouped_results = group_by_column(selected_data, group_by, selected_column_name, aggregate_function)\n",
    "    else:\n",
    "        grouped_results = selected_data\n",
    "\n",
    "    # Sorting the results if sort_order is specified\n",
    "    if sort_order is not None and aggregate_function is not None:\n",
    "        try:\n",
    "            if sort_order.lower() == 'asc':\n",
    "                grouped_results.sort(key=lambda x: (x[aggregate_function.capitalize()] is None, x[aggregate_function.capitalize()]))\n",
    "            elif sort_order.lower() == 'desc':\n",
    "                grouped_results.sort(key=lambda x: (x[aggregate_function.capitalize()] is None, x[aggregate_function.capitalize()]), reverse=True)\n",
    "        except KeyError:\n",
    "            print(\"Invalid sort_order or aggregate_function\")\n",
    "\n",
    "    return grouped_results\n",
    "\n",
    "# Function to evaluate the WHERE condition\n",
    "def evaluate_condition(row, condition):\n",
    "    operators = ['>', '<', '>=', '<=', '=', '!=']\n",
    "    for operator in operators:\n",
    "        if operator in condition:\n",
    "            column, value = condition.split(operator)\n",
    "            column = column.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if operator == '=':\n",
    "                operator = '=='\n",
    "\n",
    "            if '*' in value:\n",
    "                # If the value contains \"*\", perform wildcard matching\n",
    "                value_pattern = value.replace('*', '.*')\n",
    "                if re.match(value_pattern, row.get(column)) is not None:\n",
    "                    return True\n",
    "            else:\n",
    "                # Perform standard comparison with conversion for \"valuation\" column\n",
    "                if column in row:\n",
    "                    if column == \"valuation\":\n",
    "                        try:\n",
    "                            row_value = float(row[column])\n",
    "                            value = float(value)\n",
    "                            if eval(f'{row_value} {operator} {value}'):\n",
    "                                return True\n",
    "                        except ValueError:\n",
    "                            # Handle the case where the \"valuation\" column cannot be converted to a numeric type\n",
    "                            pass\n",
    "                    else:\n",
    "                        if eval(f'\"{row[column]}\" {operator} \"{value}\"'):\n",
    "                            return True\n",
    "    return False\n",
    "\n",
    "# Function group_by_column with the additional parameter for selected column name\n",
    "def group_by_column(data, group_column_name, selected_column_name, aggregate_function):\n",
    "    grouped_data = {}\n",
    "    for row in data:\n",
    "        key = row.get(group_column_name)\n",
    "        if key not in grouped_data:\n",
    "            grouped_data[key] = []\n",
    "\n",
    "        if aggregate_function == 'count':\n",
    "            # For count, include all rows\n",
    "            grouped_data[key].append(row.get(selected_column_name))\n",
    "        else:\n",
    "            # Convert the selected_column to a numeric value, or use None if conversion fails\n",
    "            value = row.get(selected_column_name)\n",
    "            try:\n",
    "                value = float(value) if value else None\n",
    "            except ValueError:\n",
    "                value = None\n",
    "\n",
    "            grouped_data[key].append(value)\n",
    "\n",
    "    grouped_results = []\n",
    "    for key, values in grouped_data.items():\n",
    "        if aggregate_function == 'count':\n",
    "            result = len(values)\n",
    "        else:\n",
    "            # Filter out None values for other aggregations\n",
    "            filtered_values = [v for v in values if v is not None]\n",
    "\n",
    "            if aggregate_function == 'max':\n",
    "                result = max(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'min':\n",
    "                result = min(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'avg':\n",
    "                result = np.mean(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'sum':\n",
    "                result = sum(filtered_values) if filtered_values else None\n",
    "\n",
    "        grouped_results.append({group_column_name: key, aggregate_function.capitalize() if aggregate_function else \"Data\": result})\n",
    "\n",
    "    return grouped_results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "dataset_path = 'Building_Permits.csv'\n",
    "# dataset_path = 'permit_data.csv'\n",
    "\n",
    "# Example usage \n",
    "user_input = \"select record_status record_id from data\"\n",
    "group_by_column_name = \"record_status\"\n",
    "aggregate_function = \"count\"\n",
    "sort_order = 'asc'\n",
    "results = query_permit_data(dataset_path, user_input, group_by=group_by_column_name, aggregate_function=aggregate_function, sort_order=sort_order)\n",
    "\n",
    "for result in results:\n",
    "    for column, value in result.items():\n",
    "        print(f'{column}: {value}')\n",
    "    print('-' * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Kristian - To add:\n",
    "\n",
    "# Today:\n",
    "\n",
    "# dates - optional\n",
    "# \"and\" in WHERE clause\n",
    "# \"done\" or quick ending for ADD data function\n",
    "\n",
    "\n",
    "# join/ chunking- indexing, hashing\n",
    "# primary keys\n",
    "# function to designate data types for columns optonal\n",
    "\n",
    "# Wednesday:\n",
    "# natural language \n",
    "# generalize \n",
    "# chunking\n",
    "\n",
    "######################################################\n",
    "\n",
    "# Brad - To add:\n",
    "\n",
    "# Today:\n",
    "# Max, Min, AVG sum\n",
    "# Group by - interference of NULL Values\n",
    "# dates - optional\n",
    "\n",
    "#Tuesday: \n",
    "# Plugging in the query function main.py\n",
    "\n",
    "# primary keys column joining restrictions -optional\n",
    "\n",
    "# Wednesday:\n",
    "# natural language \n",
    "# chunking\n",
    "\n",
    "###################################################################################################################\n",
    "# function to designate data types for columns\n",
    "# guide doc on how to intereact with database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated Code (FULL) - Working 11/15 Wednesday night\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Load CSV Data\n",
    "data = []\n",
    "csv_file_path = 'permit_data.csv'\n",
    "# csv_file_path = 'Building_Permits.csv'\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    with open(csv_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            data.append(row)\n",
    "\n",
    "def save_data():\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        fieldnames = desired_columns\n",
    "        csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        csv_writer.writeheader()\n",
    "        csv_writer.writerows(data)\n",
    "\n",
    "desired_columns = [\n",
    "    # Include columns needed for Query 1\n",
    "    \"id\", \"record_id\", \"open_date\", \"issued_date\", \"record_status\", \n",
    "    \"record_group\", \"record_type\", \"record_subtype\", \"record_category\", \n",
    "    \"primary_scope_code\", \"use\", \"homeowner_biz_owner\", \"street_address\", \n",
    "    \"city\", \"state\", \"zip_code\", \"full_address\", \"parcel_number\", \n",
    "    \"valuation\", \"floor_area\", \"contractor_name\", \"contractor_address\", \n",
    "    \"contractor_phone\", \"created_online\", \"last_updated\", \"geocoded_column\"\n",
    "]\n",
    "\n",
    "def show_columns(entry):\n",
    "    selected_columns = {}\n",
    "    for column in desired_columns:\n",
    "        selected_columns[column] = entry.get(column, \"\")\n",
    "    return selected_columns\n",
    "\n",
    "def generate_random_id():\n",
    "    while True:\n",
    "        new_id = ''.join(random.choice(string.digits) for _ in range(random.randint(3, 6)))\n",
    "        if not any(entry[\"id\"] == new_id for entry in data):\n",
    "            return new_id\n",
    "\n",
    "def add_data():\n",
    "    new_id = generate_random_id()\n",
    "    new_entry = {\"id\": new_id}\n",
    "    for field in desired_columns:\n",
    "        if field != \"id\":\n",
    "            user_input = input(f\"Enter data for '{field}' (Press Enter to leave it empty): \")\n",
    "            if user_input:\n",
    "                new_entry[field] = user_input\n",
    "            else:\n",
    "                new_entry[field] = None\n",
    "    data.append(new_entry)\n",
    "    save_data()\n",
    "    print(f\"New data with ID {new_id} added successfully.\")\n",
    "\n",
    "def delete_information():\n",
    "    permit_id = input(\"Enter Permit ID: \")\n",
    "    deleted_entry = None\n",
    "\n",
    "    for entry in data:\n",
    "        if entry[\"id\"] == permit_id:\n",
    "            print(\"Permit Information to Delete:\")\n",
    "            columns_and_values = show_columns(entry)\n",
    "            for key, value in columns_and_values.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "\n",
    "            check_deletion = input(\"Delete this entry from the database? (Y/N): \")\n",
    "            if check_deletion.lower() == \"y\":\n",
    "                deleted_entry = entry\n",
    "                data.remove(entry)\n",
    "                print(f\"Permit information for {permit_id} deleted successfully.\")\n",
    "                break\n",
    "\n",
    "    if deleted_entry:\n",
    "        save_data()\n",
    "    else:\n",
    "        print(f\"No information found for Permit ID {permit_id}.\")\n",
    "\n",
    "def modify_information():\n",
    "    permit_id = input(\"Enter Permit ID: \")\n",
    "    modified_entry = None\n",
    "\n",
    "    for entry in data:\n",
    "        if entry[\"id\"] == permit_id:\n",
    "            print(\"Permit Information to Modify:\")\n",
    "            columns_and_values = show_columns(entry)\n",
    "            for key, value in columns_and_values.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "\n",
    "            while True:\n",
    "                attribute = input(\"Enter the attribute from above to modify (or 'done' to exit): \")\n",
    "                if attribute == 'done':\n",
    "                    break\n",
    "                if attribute in entry:\n",
    "                    new_value = input(f\"Enter new value for {attribute}: \")\n",
    "                    entry[attribute] = new_value\n",
    "                    print(f\"{attribute} for Permit ID {permit_id} updated successfully.\")\n",
    "                else:\n",
    "                    print(f\"Invalid attribute: {attribute}\")\n",
    "\n",
    "            modified_entry = entry\n",
    "            save_data()\n",
    "            print(f\"Permit information for {permit_id} updated successfully.\")\n",
    "            break\n",
    "\n",
    "    if not modified_entry:\n",
    "        print(f\"No information found for Permit ID {permit_id}.\")\n",
    "\n",
    "\n",
    "# Function to evaluate the WHERE condition\n",
    "def evaluate_condition(row, condition):\n",
    "    operators = ['>', '<', '>=', '<=', '=', '!=']\n",
    "    for operator in operators:\n",
    "        if operator in condition:\n",
    "            column, value = condition.split(operator)\n",
    "            column = column.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if operator == '=':\n",
    "                operator = '=='\n",
    "\n",
    "            if '*' in value:\n",
    "                # If the value contains \"*\", perform wildcard matching\n",
    "                value_pattern = value.replace('*', '.*')\n",
    "                if re.match(value_pattern, row.get(column)) is not None:\n",
    "                    return True\n",
    "            else:\n",
    "                # Perform standard comparison with conversion for \"valuation\" column\n",
    "                if column in row:\n",
    "                    if column == \"valuation\":\n",
    "                        try:\n",
    "                            row_value = float(row[column])\n",
    "                            value = float(value)\n",
    "                            if eval(f'{row_value} {operator} {value}'):\n",
    "                                return True\n",
    "                        except ValueError:\n",
    "                            # Handle the case where the \"valuation\" column cannot be converted to a numeric type\n",
    "                            pass\n",
    "                    else:\n",
    "                        if eval(f'\"{row[column]}\" {operator} \"{value}\"'):\n",
    "                            return True\n",
    "    return False\n",
    "\n",
    "# Function group_by_column with the additional parameter for selected column name\n",
    "def group_by_column(data, group_column_name, selected_column_name, aggregate_function):\n",
    "    grouped_data = {}\n",
    "    for row in data:\n",
    "        key = row.get(group_column_name)\n",
    "        if key not in grouped_data:\n",
    "            grouped_data[key] = []\n",
    "\n",
    "        if aggregate_function == 'count':\n",
    "            # For count, include all rows\n",
    "            grouped_data[key].append(row.get(selected_column_name))\n",
    "        else:\n",
    "            # Convert the selected_column to a numeric value, or use None if conversion fails\n",
    "            value = row.get(selected_column_name)\n",
    "            try:\n",
    "                value = float(value) if value else None\n",
    "            except ValueError:\n",
    "                value = None\n",
    "\n",
    "            grouped_data[key].append(value)\n",
    "\n",
    "    grouped_results = []\n",
    "    for key, values in grouped_data.items():\n",
    "        if aggregate_function == 'count':\n",
    "            result = len(values)\n",
    "        else:\n",
    "            # Filter out None values for other aggregations\n",
    "            filtered_values = [v for v in values if v is not None]\n",
    "\n",
    "            if aggregate_function == 'max':\n",
    "                result = max(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'min':\n",
    "                result = min(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'avg':\n",
    "                result = np.mean(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'sum':\n",
    "                result = sum(filtered_values) if filtered_values else None\n",
    "\n",
    "        grouped_results.append({group_column_name: key, aggregate_function.capitalize() if aggregate_function else \"Data\": result})\n",
    "\n",
    "    return grouped_results\n",
    "\n",
    "\n",
    "def query_permit_data(dataset_path, user_input, group_by=None, aggregate_function=None, sort_order=None):\n",
    "    # Read the CSV dataset\n",
    "    data = []\n",
    "    # with open(dataset_path, 'r', newline='') as csvfile:\n",
    "    with open(dataset_path, 'r', newline='', encoding='utf-8') as csvfile:  # If utf-8 doesn't work, try 'cp1252' or 'latin1'\n",
    "\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            data.append(row)\n",
    "\n",
    "    # Split the user input into query components\n",
    "    query_components = re.split(r'\\s+', user_input)\n",
    "    select_clause = query_components[1:query_components.index('from')]\n",
    "    from_clause = query_components[query_components.index('from') + 1]\n",
    "    where_index = query_components.index('where') if 'where' in query_components else None\n",
    "\n",
    "    # Filter the dataset based on the WHERE clause if present\n",
    "    filtered_data = data\n",
    "    if where_index is not None:\n",
    "        where_clause = ' '.join(query_components[where_index + 1:])\n",
    "        filtered_data = [row for row in data if evaluate_condition(row, where_clause)]\n",
    "\n",
    "    # Extract the SELECTed columns from the filtered data\n",
    "    selected_data = []\n",
    "    for row in filtered_data:\n",
    "        row_data = {}\n",
    "        for column in select_clause:\n",
    "            row_data[column] = row[column]\n",
    "        selected_data.append(row_data)\n",
    "\n",
    "    selected_column_name = select_clause[-1]  # Assumes the last item in select_clause is the column to be aggregated\n",
    "\n",
    "    if group_by is not None:\n",
    "        grouped_results = group_by_column(selected_data, group_by, selected_column_name, aggregate_function)\n",
    "    else:\n",
    "        grouped_results = selected_data\n",
    "\n",
    "    # Sorting the results if sort_order is specified\n",
    "    if sort_order is not None and aggregate_function is not None:\n",
    "        try:\n",
    "            if sort_order.lower() == 'asc':\n",
    "                grouped_results.sort(key=lambda x: (x[aggregate_function.capitalize()] is None, x[aggregate_function.capitalize()]))\n",
    "            elif sort_order.lower() == 'desc':\n",
    "                grouped_results.sort(key=lambda x: (x[aggregate_function.capitalize()] is None, x[aggregate_function.capitalize()]), reverse=True)\n",
    "        except KeyError:\n",
    "            print(\"Invalid sort_order or aggregate_function\")\n",
    "\n",
    "    return grouped_results\n",
    "\n",
    "# Load initial data\n",
    "load_data()\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "    print(\"\\nOptions:\")\n",
    "    print(\"1. Show columns\")\n",
    "    print(\"2. Add data\")\n",
    "    print(\"3. Modify data\")\n",
    "    print(\"4. Delete data\")\n",
    "    print(\"5. Query data\")  # Option for Query 1\n",
    "    print(\"6. Exit\")\n",
    "\n",
    "    choice = input(\"Permits db> \").lower()\n",
    "\n",
    "    if any(keyword in choice for keyword in [\"columns\", \"show\", \"display\"]):\n",
    "        if data:\n",
    "            entry_id = input(\"Enter the ID of the data to display: \")\n",
    "            entry = next((entry for entry in data if entry[\"id\"] == entry_id), None)\n",
    "            if entry:\n",
    "                columns_and_values = show_columns(entry)\n",
    "                for key, value in columns_and_values.items():\n",
    "                    print(f\"{key}: {value}\")\n",
    "            else:\n",
    "                print(f\"No information found for ID {entry_id}.\")\n",
    "        else:\n",
    "            print(\"The database is empty.\")\n",
    "\n",
    "    elif \"add\" in choice:\n",
    "        add_data()\n",
    "\n",
    "    elif \"modify\" in choice:\n",
    "        modify_information()\n",
    "\n",
    "    elif \"delete\" in choice:\n",
    "        delete_information()\n",
    "\n",
    "    elif \"query\" in choice:\n",
    "        user_query = input(\"Enter the SQL-like query: \")\n",
    "        group_by_column_name = input(\"Enter column to group by (or leave blank for none): \")\n",
    "        aggregate_function = input(\"Enter aggregate function (max, min, avg, sum, count, or leave blank for none): \")\n",
    "        sort_order = input(\"Enter sort order (asc, desc, or leave blank for none): \")\n",
    "\n",
    "        group_by_column_name = None if not group_by_column_name else group_by_column_name\n",
    "        aggregate_function = None if not aggregate_function else aggregate_function\n",
    "        sort_order = None if not sort_order else sort_order\n",
    "\n",
    "        results = query_permit_data(csv_file_path, user_query, group_by=group_by_column_name, aggregate_function=aggregate_function, sort_order=sort_order)\n",
    "        for result in results:\n",
    "            for column, value in result.items():\n",
    "                print(f'{column}: {value}')\n",
    "            print('-' * 20)\n",
    "\n",
    "    elif \"exit\" in choice:\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid choice. Please select a valid option.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
