{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_id: PDS2013-RESALT-007197\n",
      "parcel_number: 594-240-39-00\n",
      "primary_scope_code: 8002 - REN - (Online) RES Roof Mt Solar PV No Meter Upgrade (HRA)\n",
      "valuation: \n",
      "record_id: PDS2013-RESALT-007200\n",
      "parcel_number: 590-441-16-00\n",
      "primary_scope_code: 8002 - REN - (Online) RES Roof Mt Solar PV No Meter Upgrade (HRA)\n",
      "valuation: \n"
     ]
    }
   ],
   "source": [
    "# Without separation between records\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Function to query the dataset based on user input\n",
    "def query_permit_data(dataset_path, user_input):\n",
    "    # Read the CSV dataset\n",
    "    data = []\n",
    "    with open(dataset_path, 'r', newline='') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            data.append(row)\n",
    "\n",
    "    # Split the user input into query components\n",
    "    query_components = re.split(r'\\s+', user_input)\n",
    "    select_clause = query_components[1:query_components.index('from')]\n",
    "    from_clause = query_components[query_components.index('from') + 1]\n",
    "    where_clause = ' '.join(query_components[query_components.index('where') + 1:])\n",
    "\n",
    "    # Filter the dataset based on the WHERE clause\n",
    "    filtered_data = []\n",
    "    for row in data:\n",
    "        if evaluate_condition(row, where_clause):\n",
    "            filtered_data.append(row)\n",
    "\n",
    "    # Extract the SELECTed columns from the filtered data\n",
    "    result = []\n",
    "    for row in filtered_data:\n",
    "        selected_data = {}\n",
    "        for column in select_clause:\n",
    "            selected_data[column] = row[column]\n",
    "        result.append(selected_data)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Function to evaluate the WHERE condition\n",
    "def evaluate_condition(row, condition):\n",
    "    operators = ['>', '<', '>=', '<=', '=', '!=']\n",
    "    for operator in operators:\n",
    "        if operator in condition:\n",
    "            column, value = condition.split(operator)\n",
    "            column = column.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if operator == '=':\n",
    "                operator = '=='\n",
    "\n",
    "            if column in row and eval(f'\"{row[column]}\" {operator} \"{value}\"'):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "dataset_path = 'xpermit_data.csv'\n",
    "user_input = \"select record_id parcel_number primary_scope_code valuation from data where primary_scope_code = 8002 - REN - (Online) RES Roof Mt Solar PV No Meter Upgrade (HRA)\"\n",
    "results = query_permit_data(dataset_path, user_input)\n",
    "for result in results:\n",
    "    for column, value in result.items():\n",
    "        print(f'{column}: {value}')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_id: PDS2013-MHPARK-000239\n",
      "parcel_number: 185-332-09-66\n",
      "primary_scope_code: 1158 - M.H.P. - 902 - Installation - New Or Existing\n",
      "valuation: \n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007205\n",
      "parcel_number: 104-412-03-00\n",
      "primary_scope_code: 8020 - ACC - Mechanical Only\n",
      "valuation: 12601.08\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007202\n",
      "parcel_number: 379-171-02-00\n",
      "primary_scope_code: 8040 - ACC - Electrical & Mechanical\n",
      "valuation: 8874\n",
      "--------------------\n",
      "record_id: PDS2013-RESACC-000628\n",
      "parcel_number: 266-360-22-00\n",
      "primary_scope_code: 3313 - ACC - Retaining Walls\n",
      "valuation: \n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007208\n",
      "parcel_number: 222-501-03-00\n",
      "primary_scope_code: 8020 - ACC - Mechanical Only\n",
      "valuation: 11567.52\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007188\n",
      "parcel_number: 217-031-09-00\n",
      "primary_scope_code: 4340 - RES - Addition/Alter To SFD/Duplex\n",
      "valuation: 85034.84\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007180\n",
      "parcel_number: 491-520-41-00\n",
      "primary_scope_code: 8000 - ACC - Electrical Only\n",
      "valuation: \n",
      "--------------------\n",
      "record_id: PDS2013-COMALT-000671\n",
      "parcel_number: 678-291-35-00\n",
      "primary_scope_code: 8000 - ACC - Electrical Only\n",
      "valuation: \n",
      "--------------------\n",
      "record_id: PDS2008-1001-20080170\n",
      "parcel_number: 606-150-30-00\n",
      "primary_scope_code: 1010 - RES - New Primary Residential Structure\n",
      "valuation: 499101.78\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007218\n",
      "parcel_number: 590-420-07-00\n",
      "primary_scope_code: 3310 - RES - Misc. (Antennas, Tennis Cts, Gazebo)\n",
      "valuation: 1530\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007194\n",
      "parcel_number: 509-150-57-00\n",
      "primary_scope_code: 8003 - REN - Solar Photovoltaic Residential (HRA)\n",
      "valuation: \n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007215\n",
      "parcel_number: 506-061-08-00\n",
      "primary_scope_code: 8000 - ACC - Electrical Only\n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n",
      "--------------------\n",
      "record_id: \n",
      "parcel_number: \n",
      "primary_scope_code: \n",
      "valuation: \n"
     ]
    }
   ],
   "source": [
    "# With separation between records\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Function to query the dataset based on user input\n",
    "def query_permit_data(dataset_path, user_input):\n",
    "    # Read the CSV dataset\n",
    "    data = []\n",
    "    with open(dataset_path, 'r', newline='') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            data.append(row)\n",
    "\n",
    "    # Split the user input into query components\n",
    "    query_components = re.split(r'\\s+', user_input)\n",
    "    select_clause = query_components[1:query_components.index('from')]\n",
    "    from_clause = query_components[query_components.index('from') + 1]\n",
    "    where_clause = ' '.join(query_components[query_components.index('where') + 1:])\n",
    "\n",
    "    # Filter the dataset based on the WHERE clause\n",
    "    filtered_data = []\n",
    "    for row in data:\n",
    "        if evaluate_condition(row, where_clause):\n",
    "            filtered_data.append(row)\n",
    "\n",
    "    # Extract the SELECTed columns from the filtered data\n",
    "    result = []\n",
    "    for row in filtered_data:\n",
    "        selected_data = {}\n",
    "        for column in select_clause:\n",
    "            selected_data[column] = row[column]\n",
    "        result.append(selected_data)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Function to evaluate the WHERE condition\n",
    "def evaluate_condition(row, condition):\n",
    "    operators = ['>', '<', '>=', '<=', '=', '!=']\n",
    "    for operator in operators:\n",
    "        if operator in condition:\n",
    "            column, value = condition.split(operator)\n",
    "            column = column.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if operator == '=':\n",
    "                operator = '=='\n",
    "\n",
    "            if column in row and eval(f'\"{row[column]}\" {operator} \"{value}\"'):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "dataset_path = 'xpermit_data.csv'\n",
    "user_input = \"select record_id parcel_number primary_scope_code valuation from data where primary_scope_code != 8002 - REN - (Online) RES Roof Mt Solar PV No Meter Upgrade (HRA)\"\n",
    "results = query_permit_data(dataset_path, user_input)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    for column, value in result.items():\n",
    "        print(f'{column}: {value}')\n",
    "    # Add a separator between records except for the last one\n",
    "    if i < len(results) - 1:\n",
    "        print('-' * 20)  # Use any separator you prefer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_id: PDS2013-RESALT-007197\n",
      "parcel_number: 594-240-39-00\n",
      "primary_scope_code: 8002 - REN - (Online) RES Roof Mt Solar PV No Meter Upgrade (HRA)\n",
      "valuation: \n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007200\n",
      "parcel_number: 590-441-16-00\n",
      "primary_scope_code: 8002 - REN - (Online) RES Roof Mt Solar PV No Meter Upgrade (HRA)\n",
      "valuation: \n"
     ]
    }
   ],
   "source": [
    "# Optional WHERE clause - WORKING\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Function to query the dataset based on user input\n",
    "def query_permit_data(dataset_path, user_input):\n",
    "    # Read the CSV dataset\n",
    "    data = []\n",
    "    with open(dataset_path, 'r', newline='') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            data.append(row)\n",
    "\n",
    "    # Split the user input into query components\n",
    "    query_components = re.split(r'\\s+', user_input)\n",
    "    select_clause = query_components[1:query_components.index('from')]\n",
    "    from_clause = query_components[query_components.index('from') + 1]\n",
    "    where_index = query_components.index('where') if 'where' in query_components else None\n",
    "\n",
    "    # Filter the dataset based on the WHERE clause if present\n",
    "    filtered_data = data\n",
    "    if where_index is not None:\n",
    "        where_clause = ' '.join(query_components[where_index + 1:])\n",
    "        filtered_data = [row for row in data if evaluate_condition(row, where_clause)]\n",
    "\n",
    "    # Extract the SELECTed columns from the filtered data\n",
    "    result = []\n",
    "    for row in filtered_data:\n",
    "        selected_data = {}\n",
    "        for column in select_clause:\n",
    "            selected_data[column] = row[column]\n",
    "        result.append(selected_data)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Function to evaluate the WHERE condition\n",
    "def evaluate_condition(row, condition):\n",
    "    operators = ['>', '<', '>=', '<=', '=', '!=']\n",
    "    for operator in operators:\n",
    "        if operator in condition:\n",
    "            column, value = condition.split(operator)\n",
    "            column = column.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if operator == '=':\n",
    "                operator = '=='\n",
    "\n",
    "            if '*' in value:\n",
    "                # If the value contains \"*\", perform wildcard matching\n",
    "                value_pattern = value.replace('*', '.*')\n",
    "                if re.match(value_pattern, row.get(column)) is not None:\n",
    "                    return True\n",
    "            else:\n",
    "                # Perform standard comparison\n",
    "                if column in row and eval(f'\"{row[column]}\" {operator} \"{value}\"'):\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "dataset_path = 'xpermit_data.csv'\n",
    "user_input = \"select record_id parcel_number primary_scope_code valuation from data where primary_scope_code = 8002 - REN - (Online) RES Roof Mt Solar PV No Meter Upgrade (HRA)\"\n",
    "# user_input = \"select record_id parcel_number primary_scope_code valuation from data where parcel_number = 594-240-39-00\"\n",
    "results = query_permit_data(dataset_path, user_input)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    for column, value in result.items():\n",
    "        print(f'{column}: {value}')\n",
    "    # Add a separator between records except for the last one\n",
    "    if i < len(results) - 1:\n",
    "        print('-' * 20)  # Use any separator you prefer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_id: PDS2013-RESALT-007197\n",
      "record_status: Issued Invalid Expired\n",
      "--------------------\n",
      "record_id: PDS2013-MHPARK-000239\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007205\n",
      "record_status: Issued Invalid Expired\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007202\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2013-RESACC-000628\n",
      "record_status: Issued Expired\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007208\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007188\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007180\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2013-COMALT-000671\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2008-1001-20080170\n",
      "record_status: PC Expired\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007218\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007194\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007200\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: PDS2013-RESALT-007215\n",
      "record_status: Completed\n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n",
      "record_id: \n",
      "record_status: \n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# 11/13/ - GROUP BY - Working, shows count\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Function to query the dataset based on user input\n",
    "def query_permit_data(dataset_path, user_input, group_by=None):\n",
    "    # Read the CSV dataset\n",
    "    data = []\n",
    "    with open(dataset_path, 'r', newline='') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            data.append(row)\n",
    "\n",
    "    # Split the user input into query components\n",
    "    query_components = re.split(r'\\s+', user_input)\n",
    "    select_clause = query_components[1:query_components.index('from')]\n",
    "    from_clause = query_components[query_components.index('from') + 1]\n",
    "    where_index = query_components.index('where') if 'where' in query_components else None\n",
    "\n",
    "    # Filter the dataset based on the WHERE clause if present\n",
    "    filtered_data = data\n",
    "    if where_index is not None:\n",
    "        where_clause = ' '.join(query_components[where_index + 1:])\n",
    "        filtered_data = [row for row in data if evaluate_condition(row, where_clause)]\n",
    "\n",
    "    # Extract the SELECTed columns from the filtered data\n",
    "    selected_data = []\n",
    "    for row in filtered_data:\n",
    "        row_data = {}\n",
    "        for column in select_clause:\n",
    "            row_data[column] = row[column]\n",
    "        selected_data.append(row_data)\n",
    "\n",
    "    if group_by is not None:\n",
    "        grouped_results = group_by_column(selected_data, group_by)\n",
    "    else:\n",
    "        grouped_results = selected_data\n",
    "\n",
    "    return grouped_results\n",
    "\n",
    "# Function to evaluate the WHERE condition\n",
    "def evaluate_condition(row, condition):\n",
    "    operators = ['>', '<', '>=', '<=', '=', '!=']\n",
    "    for operator in operators:\n",
    "        if operator in condition:\n",
    "            column, value = condition.split(operator)\n",
    "            column = column.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if operator == '=':\n",
    "                operator = '=='\n",
    "\n",
    "            if '*' in value:\n",
    "                # If the value contains \"*\", perform wildcard matching\n",
    "                value_pattern = value.replace('*', '.*')\n",
    "                if re.match(value_pattern, row.get(column)) is not None:\n",
    "                    return True\n",
    "            else:\n",
    "                # Perform standard comparison with conversion for \"valuation\" column\n",
    "                if column in row:\n",
    "                    if column == \"valuation\":\n",
    "                        try:\n",
    "                            row_value = float(row[column])\n",
    "                            value = float(value)\n",
    "                            if eval(f'{row_value} {operator} {value}'):\n",
    "                                return True\n",
    "                        except ValueError:\n",
    "                            # Handle the case where the \"valuation\" column cannot be converted to a numeric type\n",
    "                            pass\n",
    "                    else:\n",
    "                        if eval(f'\"{row[column]}\" {operator} \"{value}\"'):\n",
    "                            return True\n",
    "    return False\n",
    "\n",
    "# Function to group results by a specified column and count occurrences\n",
    "def group_by_column(data, column_name):\n",
    "    grouped_data = {}\n",
    "    for row in data:\n",
    "        value = row.get(column_name)\n",
    "        if value not in grouped_data:\n",
    "            grouped_data[value] = 1\n",
    "        else:\n",
    "            grouped_data[value] += 1\n",
    "\n",
    "    grouped_results = []\n",
    "    for value, count in grouped_data.items():\n",
    "        grouped_results.append({column_name: value, 'Count': count})  # Include count in the result\n",
    "\n",
    "    return grouped_results\n",
    "\n",
    "# Example usage\n",
    "dataset_path = 'xpermit_data.csv'\n",
    "\n",
    "# user_input = \"select record_id parcel_number open_date primary_scope_code valuation from data where parcel_number = 594-240-39-00\"\n",
    "# user_input = \"select record_id parcel_number primary_scope_code valuation from data where valuation >= 400000\"\n",
    "# user_input = \"select record_id parcel_number primary_scope_code valuation record_category from data where record_category = Residential*\"\n",
    "# user_input = \"select record_status record_id parcel_number primary_scope_code valuation record_category from data where record_status = Com*\"\n",
    "user_input = \"select record_id record_status from data\"\n",
    "group_by_column_name = None\n",
    "\n",
    "# user_input = \"select record_status from data\"\n",
    "# group_by_column_name = 'record_status'  # Set to None if you don't want to group\n",
    "results = query_permit_data(dataset_path, user_input, group_by=group_by_column_name)\n",
    "\n",
    "for result in results:\n",
    "    for column, value in result.items():\n",
    "        print(f'{column}: {value}')\n",
    "    print('-' * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_status: Issued Expired\n",
      "valuation: \n",
      "primary_scope_code: 3313 - ACC - Retaining Walls\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# all aggregations - working\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Function to query the dataset based on user input\n",
    "def query_permit_data(dataset_path, user_input, group_by=None, aggregate_function=None):\n",
    "    # Read the CSV dataset\n",
    "    data = []\n",
    "    with open(dataset_path, 'r', newline='') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            data.append(row)\n",
    "\n",
    "    # Split the user input into query components\n",
    "    query_components = re.split(r'\\s+', user_input)\n",
    "    select_clause = query_components[1:query_components.index('from')]\n",
    "    from_clause = query_components[query_components.index('from') + 1]\n",
    "    where_index = query_components.index('where') if 'where' in query_components else None\n",
    "\n",
    "    # Filter the dataset based on the WHERE clause if present\n",
    "    filtered_data = data\n",
    "    if where_index is not None:\n",
    "        where_clause = ' '.join(query_components[where_index + 1:])\n",
    "        filtered_data = [row for row in data if evaluate_condition(row, where_clause)]\n",
    "\n",
    "    # Extract the SELECTed columns from the filtered data\n",
    "    selected_data = []\n",
    "    for row in filtered_data:\n",
    "        row_data = {}\n",
    "        for column in select_clause:\n",
    "            row_data[column] = row[column]\n",
    "        selected_data.append(row_data)\n",
    "\n",
    "    if group_by is not None:\n",
    "        grouped_results = group_by_column(selected_data, group_by, aggregate_function)\n",
    "    else:\n",
    "        grouped_results = selected_data\n",
    "\n",
    "    return grouped_results\n",
    "\n",
    "# Function to evaluate the WHERE condition\n",
    "def evaluate_condition(row, condition):\n",
    "    operators = ['>', '<', '>=', '<=', '=', '!=']\n",
    "    for operator in operators:\n",
    "        if operator in condition:\n",
    "            column, value = condition.split(operator)\n",
    "            column = column.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if operator == '=':\n",
    "                operator = '=='\n",
    "\n",
    "            if '*' in value:\n",
    "                # If the value contains \"*\", perform wildcard matching\n",
    "                value_pattern = value.replace('*', '.*')\n",
    "                if re.match(value_pattern, row.get(column)) is not None:\n",
    "                    return True\n",
    "            else:\n",
    "                # Perform standard comparison with conversion for \"valuation\" column\n",
    "                if column in row:\n",
    "                    if column == \"valuation\":\n",
    "                        try:\n",
    "                            row_value = float(row[column])\n",
    "                            value = float(value)\n",
    "                            if eval(f'{row_value} {operator} {value}'):\n",
    "                                return True\n",
    "                        except ValueError:\n",
    "                            # Handle the case where the \"valuation\" column cannot be converted to a numeric type\n",
    "                            pass\n",
    "                    else:\n",
    "                        if eval(f'\"{row[column]}\" {operator} \"{value}\"'):\n",
    "                            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def group_by_column(data, column_name, aggregate_function):\n",
    "    grouped_data = {}\n",
    "    for row in data:\n",
    "        key = row.get(column_name)\n",
    "        if key not in grouped_data:\n",
    "            grouped_data[key] = []\n",
    "\n",
    "        # Convert the valuation to a numeric value, or use None if conversion fails\n",
    "        valuation = row.get('valuation')\n",
    "        try:\n",
    "            valuation = float(valuation) if valuation else None\n",
    "        except ValueError:\n",
    "            valuation = None\n",
    "\n",
    "        grouped_data[key].append(valuation)\n",
    "\n",
    "    grouped_results = []\n",
    "    for key, values in grouped_data.items():\n",
    "        # Filter out None values for aggregation\n",
    "        filtered_values = [v for v in values if v is not None]\n",
    "\n",
    "        if aggregate_function:\n",
    "            if aggregate_function == 'max':\n",
    "                result = max(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'min':\n",
    "                result = min(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'avg':\n",
    "                result = np.mean(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'sum':\n",
    "                result = sum(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'count':\n",
    "                result = len(filtered_values)\n",
    "        else:\n",
    "            result = filtered_values\n",
    "\n",
    "        grouped_results.append({column_name: key, aggregate_function.capitalize() if aggregate_function else \"Data\": result})\n",
    "\n",
    "    return grouped_results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "dataset_path = 'xpermit_data.csv'\n",
    "user_input = \"select record_status valuation primary_scope_code from data where parcel_number = 266-360-22*\"\n",
    "# group_by_column_name = \"record_status\"\n",
    "# aggregate_function = \"min\"  # Change to None for no aggregation or choose 'max', 'min', 'avg', or 'sum'\n",
    "group_by_column_name = None\n",
    "aggregate_function = None \n",
    "\n",
    "results = query_permit_data(dataset_path, user_input, group_by=group_by_column_name, aggregate_function=aggregate_function)\n",
    "\n",
    "\n",
    "for result in results:\n",
    "    for column, value in result.items():\n",
    "        print(f'{column}: {value}')\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_status: Completed\n",
      "Count: 4\n",
      "--------------------\n",
      "record_status: Issued Invalid Expired\n",
      "Count: 1\n",
      "--------------------\n",
      "record_status: PC Expired\n",
      "Count: 1\n",
      "--------------------\n",
      "record_status: Issued Expired\n",
      "Count: 0\n",
      "--------------------\n",
      "record_status: \n",
      "Count: 0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# sort - testing\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Function to query the dataset based on user input\n",
    "def query_permit_data(dataset_path, user_input, group_by=None, aggregate_function=None, sort_order=None):\n",
    "    # Read the CSV dataset\n",
    "    data = []\n",
    "    with open(dataset_path, 'r', newline='') as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile)\n",
    "        for row in csvreader:\n",
    "            data.append(row)\n",
    "\n",
    "    # Split the user input into query components\n",
    "    query_components = re.split(r'\\s+', user_input)\n",
    "    select_clause = query_components[1:query_components.index('from')]\n",
    "    from_clause = query_components[query_components.index('from') + 1]\n",
    "    where_index = query_components.index('where') if 'where' in query_components else None\n",
    "\n",
    "    # Filter the dataset based on the WHERE clause if present\n",
    "    filtered_data = data\n",
    "    if where_index is not None:\n",
    "        where_clause = ' '.join(query_components[where_index + 1:])\n",
    "        filtered_data = [row for row in data if evaluate_condition(row, where_clause)]\n",
    "\n",
    "    # Extract the SELECTed columns from the filtered data\n",
    "    selected_data = []\n",
    "    for row in filtered_data:\n",
    "        row_data = {}\n",
    "        for column in select_clause:\n",
    "            row_data[column] = row[column]\n",
    "        selected_data.append(row_data)\n",
    "\n",
    "    if group_by is not None:\n",
    "        grouped_results = group_by_column(selected_data, group_by, aggregate_function)\n",
    "    else:\n",
    "        grouped_results = selected_data\n",
    "\n",
    "    # Sorting the results if sort_order is specified\n",
    "    if sort_order is not None and aggregate_function is not None:\n",
    "        try:\n",
    "            if sort_order.lower() == 'asc':\n",
    "                grouped_results.sort(key=lambda x: (x[aggregate_function.capitalize()] is None, x[aggregate_function.capitalize()]))\n",
    "            elif sort_order.lower() == 'desc':\n",
    "                grouped_results.sort(key=lambda x: (x[aggregate_function.capitalize()] is None, x[aggregate_function.capitalize()]), reverse=True)\n",
    "        except KeyError:\n",
    "            print(\"Invalid sort_order or aggregate_function\")\n",
    "\n",
    "    return grouped_results\n",
    "\n",
    "# Function to evaluate the WHERE condition\n",
    "def evaluate_condition(row, condition):\n",
    "    operators = ['>', '<', '>=', '<=', '=', '!=']\n",
    "    for operator in operators:\n",
    "        if operator in condition:\n",
    "            column, value = condition.split(operator)\n",
    "            column = column.strip()\n",
    "            value = value.strip()\n",
    "\n",
    "            if operator == '=':\n",
    "                operator = '=='\n",
    "\n",
    "            if '*' in value:\n",
    "                # If the value contains \"*\", perform wildcard matching\n",
    "                value_pattern = value.replace('*', '.*')\n",
    "                if re.match(value_pattern, row.get(column)) is not None:\n",
    "                    return True\n",
    "            else:\n",
    "                # Perform standard comparison with conversion for \"valuation\" column\n",
    "                if column in row:\n",
    "                    if column == \"valuation\":\n",
    "                        try:\n",
    "                            row_value = float(row[column])\n",
    "                            value = float(value)\n",
    "                            if eval(f'{row_value} {operator} {value}'):\n",
    "                                return True\n",
    "                        except ValueError:\n",
    "                            # Handle the case where the \"valuation\" column cannot be converted to a numeric type\n",
    "                            pass\n",
    "                    else:\n",
    "                        if eval(f'\"{row[column]}\" {operator} \"{value}\"'):\n",
    "                            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def group_by_column(data, column_name, aggregate_function):\n",
    "    grouped_data = {}\n",
    "    for row in data:\n",
    "        key = row.get(column_name)\n",
    "        if key not in grouped_data:\n",
    "            grouped_data[key] = []\n",
    "\n",
    "        # Convert the valuation to a numeric value, or use None if conversion fails\n",
    "        valuation = row.get('valuation')\n",
    "        try:\n",
    "            valuation = float(valuation) if valuation else None\n",
    "        except ValueError:\n",
    "            valuation = None\n",
    "\n",
    "        grouped_data[key].append(valuation)\n",
    "\n",
    "    grouped_results = []\n",
    "    for key, values in grouped_data.items():\n",
    "        # Filter out None values for aggregation\n",
    "        filtered_values = [v for v in values if v is not None]\n",
    "\n",
    "        if aggregate_function:\n",
    "            if aggregate_function == 'max':\n",
    "                result = max(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'min':\n",
    "                result = min(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'avg':\n",
    "                result = np.mean(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'sum':\n",
    "                result = sum(filtered_values) if filtered_values else None\n",
    "            elif aggregate_function == 'count':\n",
    "                result = len(filtered_values)\n",
    "        else:\n",
    "            result = filtered_values\n",
    "\n",
    "        grouped_results.append({column_name: key, aggregate_function.capitalize() if aggregate_function else \"Data\": result})\n",
    "\n",
    "    return grouped_results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "dataset_path = 'xpermit_data.csv'\n",
    "user_input = \"select record_status valuation primary_scope_code from data\"\n",
    "group_by_column_name = \"record_status\"\n",
    "aggregate_function = \"count\"  # Change to None for no aggregation or choose 'max', 'min', 'avg', or 'sum'\n",
    "# group_by_column_name = None\n",
    "# aggregate_function = None \n",
    "sort_order = 'desc'  # Can be 'asc', 'desc', or None. The sorting is applied only if both sort_order and aggregate_function are specified. This is because sorting is typically applied to the aggregated results.\n",
    "results = query_permit_data(dataset_path, user_input, group_by=group_by_column_name, aggregate_function=aggregate_function, sort_order=sort_order)\n",
    "\n",
    "\n",
    "\n",
    "for result in results:\n",
    "    for column, value in result.items():\n",
    "        print(f'{column}: {value}')\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Kristian - To add:\n",
    "\n",
    "# Today:\n",
    "# Max, Min, AVG sum\n",
    "# Sort \n",
    "# Order by\n",
    "# dates - optional\n",
    "\n",
    "#Tuesday: \n",
    "# Plugging in the query function main.py\n",
    "\n",
    "# join\n",
    "# primary keys\n",
    "# function to designate data types for columns\n",
    "\n",
    "# Wednesday:\n",
    "# natural language \n",
    "# generalize \n",
    "# chunking\n",
    "\n",
    "######################################################\n",
    "\n",
    "# Brad - To add:\n",
    "\n",
    "# Today:\n",
    "# Max, Min, AVG sum\n",
    "# Group by - interference of NULL Values\n",
    "# dates - optional\n",
    "\n",
    "#Tuesday: \n",
    "# Plugging in the query function main.py\n",
    "\n",
    "# primary keys column joining restrictions -optional\n",
    "\n",
    "# Wednesday:\n",
    "# natural language \n",
    "# chunking\n",
    "\n",
    "###################################################################################################################\n",
    "# function to designate data types for columns\n",
    "# guide doc on how to intereact with database"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
